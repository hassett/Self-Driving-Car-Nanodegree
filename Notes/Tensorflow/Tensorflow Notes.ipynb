{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Introduction to Tensorflow Notes</center>\n",
    "\n",
    "<center><i>Notes from the Tensorflow section from the Self Driving Car Nanodegree on Udacity</i></center>\n",
    "\n",
    "#### Notable Tensorflow Functions\n",
    "\n",
    "1. `tf.matmul(a, b)` - the dot product of a and b\n",
    "\n",
    "\n",
    "\n",
    "#### Helpful Tips\n",
    "\n",
    "Inputs like features and labels use placeholders not variables\n",
    "\n",
    "Weights and biases use variables because they are suposed to change througout the `tf.Session()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hello_constant = tf.constant('Hellow World!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sets the variable hello_constant as a tensorflow Tensor object. In this case, hello_constant is a 0-dimensional string (0 dimensional array). Let's see what happens when you try and print it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const:0\", shape=(), dtype=string)\n",
      "<class 'tensorflow.python.framework.ops.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "print(hello_constant)\n",
    "print(type(hello_constant))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to print the constant as the string we put into the function `tf.constant()` we have to run `tf.Session()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hellow World!'\n",
      "<class 'bytes'>\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    output = sess.run(hello_constant)\n",
    "    print(output)\n",
    "    print(type(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, it still doesn't print the constant as a string. In fact it prints the output in 'byte' form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tf.constant()` objects can be more than just 0-dimensional strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "() <dtype: 'string'>\n",
      "() <dtype: 'int32'>\n",
      "(3,) <dtype: 'int32'>\n",
      "(2, 3) <dtype: 'int32'>\n"
     ]
    }
   ],
   "source": [
    "string = tf.constant('abc')\n",
    "print(string.shape, string.dtype)\n",
    "\n",
    "A = tf.constant(1234)\n",
    "print(A.shape, A.dtype)\n",
    "\n",
    "B = tf.constant([123, 456, 789])\n",
    "print(B.shape, B.dtype)\n",
    "\n",
    "C = tf.constant([[123, 456, 789], [987, 654, 321]])\n",
    "print(C.shape, C.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`obj.dtype` returns the data type of the object. As you can see, `tf.constant()` objects can be any shape. Object `C` is a 2x3 matrix and would be written as so:\n",
    "\n",
    "`|123 456 789|\n",
    " |987 654 321|`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tf.Session()` evaluates the tensor hello_constant in a session\n",
    "\n",
    "A session is what allocates computational power and operations.\n",
    "\n",
    "`sess.run()` evaluates the tensor and returns the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs with non-constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were to want to pass in a non-constant, we would first have to instatiate a `tf.placeholder()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder_19:0\", dtype=string)\n",
      "<unknown>\n",
      "<dtype: 'string'>\n",
      "\n",
      "\n",
      "Tensor(\"Placeholder_20:0\", shape=(3,), dtype=int32)\n",
      "(3,)\n",
      "<dtype: 'int32'>\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.string)\n",
    "y = tf.placeholder(tf.int32, shape = (3,))\n",
    "print(x)\n",
    "print(x.shape)\n",
    "print(x.dtype)\n",
    "print('\\n')\n",
    "print(y)\n",
    "print(y.shape)\n",
    "print(y.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting with `x`. We've instatiated a `tf.placeholder()` object with the one necessary positional argument which is the data type. We've set it to be a tf.string which we will use later. Beacuse we didn't pass in the shape parameter, we can pass in a `tf.string` object of any shape\n",
    "\n",
    "With `y`, we've instatiated it as an int32 and a shape of (3,). This means that we can only pass in an int32 with a shape of (3,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    output = sess.run(x, feed_dict={x: 'Hello World'})\n",
    "    print(output)\n",
    "    print(type(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[123 456 789]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    output = sess.run(y, feed_dict = {y: [123, 456, 789]})\n",
    "    print(output)\n",
    "    print(type(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have a placeholder and want to run something using the `tf.Session()`, we can use the feed_dict parameter to set x and y to 'Hello World' and [123, 456, 789] respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The biggest takeaway from printing out the types of the outputs is the fact that they are both numpy arrays. I believe this is because of `tf.placeholder` but I'm not sure\n",
    "\n",
    "#### CHECK WHY THEY ARE NUMPY ARRAYS ^^"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also pass both `x` and `y` in at the same time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    output_x = sess.run(x, feed_dict={x:'Test', y: [1, 2, 3]})\n",
    "    print(output_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this you can only return one of the instatiated objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Add_14:0\", shape=(), dtype=int32)\n",
      "Tensor(\"Sub_3:0\", shape=(), dtype=int32)\n",
      "Tensor(\"Mul_3:0\", shape=(), dtype=int32)\n",
      "x: 7\n",
      "y: 6\n",
      "z: 10\n"
     ]
    }
   ],
   "source": [
    "x = tf.add(5, 2)\n",
    "print(x)\n",
    "\n",
    "y = tf.subtract(10, 4)\n",
    "print(y)\n",
    "\n",
    "z = tf.multiply(2, 5)\n",
    "print(z)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print('x: ' + str(sess.run(x)))\n",
    "    print('y: ' + str(sess.run(y)))\n",
    "    print('z: ' + str(sess.run(z)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All data types have to be the same when using tensorflow math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.subtract(tf.constant(2.0), tf.constant(1))\n",
    "# TypeError: Input 'y' of 'Sub' Op has type int32 \n",
    "# that does not match type float32 of argument 'x'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead you would have to cast either the float or the int32 as either or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "sub = tf.subtract(tf.cast(tf.constant(2.1), tf.int32), tf.constant(1))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(sub))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When casting from a float to an int it <b><u>ALWAYS</u></b> rounds down\n",
    "\n",
    "QUIZ\n",
    "\n",
    "Convert this python code into tensorflow math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n"
     ]
    }
   ],
   "source": [
    "x1 = 10\n",
    "y1 = 2\n",
    "z1 = x1/y1 - 1\n",
    "\n",
    "# TODO: Print z from a session as the variable output\n",
    "output1 = z1\n",
    "print(output1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `divxy = tf.divide(x, y)` - returns a float of x/y\n",
    "2. `cast_divxy = tf.cast(divxy, tf.int32)` - casts x/y as an int32\n",
    "3. `out = tf.subtract(cast_divxy, tf.constant(1)` - subtracts int(x/y) and int 1\n",
    "4. `tf.cast(out, tf.float32)` - casts out to a float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(10)\n",
    "y = tf.constant(2)\n",
    "z = x/y - 1\n",
    "z = tf.cast(tf.subtract(tf.cast(tf.divide(x, y), tf.int32), \n",
    "                        tf.constant(1)), tf.float32)\n",
    "\n",
    "# TODO: Print z from a session as the variable output\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(z)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Linear Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tf.Variable()` creates a tensor with an initial value that can be changed, just like a normal Python variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tf.global_variables_initializer()` initialzes the state of all Variable tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tf.Variable()` lets us change the weights and biases. But we do need to pass in an initial value. We will normally initialize the weights with random numbers from a normal distrubution. This will ensure the model doesn't get stuck at the beginning of training each time.\n",
    "\n",
    "We will use `tf.truncated_normal()` to generate these random numbers from a normal distrubution. One parameter so either pass in one number or a tuple\n",
    "\n",
    "Returns a tensor with random values which is no more than 2 standard deviations from the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 120\n",
    "n_labels = 5\n",
    "weights = tf.Variable(tf.truncated_normal((n_features, n_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no need to randomize the bias. The easiest thing to do is to set the bias to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_labels = 5\n",
    "bias = tf.Variable(tf.zeros(n_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax using Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax equation:\n",
    "\n",
    "<b>sigmoid(x) = e^x/sum(x^x)</b>\n",
    "\n",
    "Have to define which axis of the array you are adding. In this case we are using the x axis so we will set the axis = 0: `np.sum(x, axis = 0)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x)/np.sum(np.exp(x), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8360188  0.11314284 0.05083836]\n"
     ]
    }
   ],
   "source": [
    "logits = [3.0, 1.0, 0.2]\n",
    "print(softmax(logits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax using Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6590012  0.24243298 0.09856589]\n"
     ]
    }
   ],
   "source": [
    "def softmax_tensorflow():\n",
    "    output = None\n",
    "    logit_data = [2.0, 1.0, 0.1]\n",
    "    logits = tf.placeholder(tf.float32)\n",
    "    \n",
    "    softmax = tf.nn.softmax(logits)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        output = sess.run(softmax, feed_dict={logits: logit_data})\n",
    "        \n",
    "    return output\n",
    "        \n",
    "print(softmax_tensorflow())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy\n",
    "\n",
    "The distance between the two probability vectors (1. softmax output, 2. onehot encoded correct output)\n",
    "\n",
    "`S = softmax output`;\n",
    "`L = onehot encoded output`\n",
    "\n",
    "`D(S, L) = cross entropy`\n",
    "\n",
    "`D(S, L) = - sum(L * log(S))`\n",
    "\n",
    "`S = softmax(Wx + b)`\n",
    "\n",
    "#### Multinomial Logistic Classification\n",
    "\n",
    "<i>`D(S(Wx+b), L)`</i> (axis=1)\n",
    "\n",
    "#### Training Loss\n",
    "\n",
    "The average cross-entropy of the entire training set\n",
    "\n",
    "n = number of features\n",
    "\n",
    "`l = 1/n * (sum(D(S(Wx+b), L))`\n",
    "\n",
    "To minimize the training loss, we must update the weights and bias. The easiest way to do this is through gradient descent\n",
    "\n",
    "<b>Next Steps:</b>\n",
    "\n",
    "1. How do you fill image pixels to this calssifier?\n",
    "2. Where do you initialize the optimization?\n",
    "\n",
    "Numerical stability in Python\n",
    "\n",
    "The numerical value of this equation should be 1.0, but the code says it is not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.95367431640625\n"
     ]
    }
   ],
   "source": [
    "a = 1000000000\n",
    "for i in range(1000000):\n",
    "    a = a + 1e-6\n",
    "print(a - 1000000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we avoid this?\n",
    "\n",
    "We want our variables to have 0 mean and equal variance whenever possible\n",
    "\n",
    "#### Normalize inputs\n",
    "\n",
    "For each color layer in an image, subtract 128 and divide that by 128\n",
    "\n",
    "`(R - 128) / 128)`\n",
    "\n",
    "`(G - 128) / 128)`\n",
    "\n",
    "`(B - 128) / 128)`\n",
    "\n",
    "#### Normalize weights\n",
    "\n",
    "Draw the weights randomly with a gaussian distribution of mean 0 and of standard deviation sigma\n",
    "\n",
    "larger sigma = large peaks and be very opinionated\n",
    "\n",
    "small sigma = uncertainty\n",
    "\n",
    "It is better to start out with uncertainty and let the model become more confident with the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring Performance and Overfitting\n",
    "\n",
    "Using training, validation, and testing sets\n",
    "\n",
    "The larger the validation set, the more precise your numbers will be\n",
    "\n",
    "The bigger the test set, the less noise there will be\n",
    "\n",
    "#### Rule of 30\n",
    "\n",
    "The more examples you have, the greater the accuracy has to change for it to be truly changing. Should be 30 examples at minimum\n",
    "\n",
    "#### Stochastic Gradient Descent\n",
    "\n",
    "Very scalable, but fundamentally pretty bad\n",
    "\n",
    "Instead of computing the actual loss we will compute an estimate. Take a random 1/1000 sliver of the training data. From there take that sliver and compute the loss and the derivative and assume it is the right direction for the rest of the training set\n",
    "\n",
    "#### Hyperparameters\n",
    "\n",
    "If you ever have problems, ALWAYS try lowering your learning rate first\n",
    "\n",
    "ADAGRAD: takes care of initial learning rate, learning rate decay, and momentum for you. Often makes learning less sensetive to hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating our Model\n",
    "\n",
    "#### todo: a breakdown\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "'''\n",
    "Don't worry about how to do this right now, this was given as a helper\n",
    "funciton\n",
    "'''\n",
    "def batches(batch_size, features, labels):\n",
    "    \"\"\"\n",
    "    Create batches of features and labels\n",
    "    :param batch_size: The batch size\n",
    "    :param features: List of features\n",
    "    :param labels: List of labels\n",
    "    :return: Batches of (Features, Labels)\n",
    "    \"\"\"\n",
    "    assert len(features) == len(labels)\n",
    "    outout_batches = []\n",
    "    \n",
    "    sample_size = len(features)\n",
    "    for start_i in range(0, sample_size, batch_size):\n",
    "        end_i = start_i + batch_size\n",
    "        batch = [features[start_i:end_i], labels[start_i:end_i]]\n",
    "        outout_batches.append(batch)\n",
    "        \n",
    "    return outout_batches\n",
    "\n",
    "def print_epoch_stats(epoch, sess, last_features, last_labels):\n",
    "    current_cost = sess.run(cost, feed_dict={features:last_features,\n",
    "                                            labels: last_labels})\n",
    "    valid_accuracy = sess.run(accuracy, feed_dict={features:valid_features,\n",
    "                                                  labels: valid_label})\n",
    "    print('Epoch: {:<4} - Cost: {:<8.3} Valid Accuracy: {:<5.3}'.format(\n",
    "            epoch, current_cost, valid_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "DO NOT RUN THIS CODE BLOCK\n",
    "\n",
    "tensorflow datasets don't seem to be working\n",
    "'''\n",
    "learning_rate = 0.001\n",
    "n_input = 784 # mnist is (28,28,1)\n",
    "n_classes = 10 # number 0-9\n",
    "\n",
    "mnist = input_data.read_data_sets('/datasets/ud730/mnist', one_hot= True)\n",
    "\n",
    "train_features = mnist.train.images\n",
    "test_features = mnist.test.images\n",
    "\n",
    "train_labels = mnist.train.labels.astype(np.float32)\n",
    "test_labels = mnist.test.labels.astype(np.float32)\n",
    "\n",
    "features = tf.placeholder(tf.float32, [None, n_input])\n",
    "labels = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "# one video has these and so does the lab but one has the one below\n",
    "#weights = tf.Variable(tf.truncated_normal((n_input, n_classes)))\n",
    "#bias = tf.Variable(tf.zeros(n_classes))\n",
    "\n",
    "weights = tf.Variable(tf.random_normal([n_input, n_classes]))\n",
    "bias = tf.Variable(tf.random_normal([n_classes]))\n",
    "\n",
    "# Wx + b\n",
    "logits = tf.add(tf.matmul(features, weights, bias))\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits,\n",
    "                                                             labels=labels))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 10\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "    \n",
    "        for batch_features, batch_labels in batches(batch_size, \n",
    "                                                    train_features, \n",
    "                                                    train_labels):\n",
    "            sess.run(optimizer, feed_dict={features: batch_features, \n",
    "                                          labels: batch_labels})\n",
    "            \n",
    "        print_epoch_stats(epoch, sess, batch_features, batch_labels)\n",
    "        \n",
    "    test_accuracy = sess.run(accuracy, feed_dict={features:test_features,\n",
    "                                                 labels:test_labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## todo for the lab\n",
    "\n",
    "1. Normalize the features\n",
    "2. Use tensorflow to create features, labels, weights, and bisaes\n",
    "3. Tune the learning rate and epochs\n",
    "\n",
    "\n",
    "1. Normalizing a Grayscale Image\n",
    "\n",
    "    - the min-max scaling method will range from 0.1 to 0.9\n",
    "    \n",
    "    - Because the images are grayscale they will range from 0 to 255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Normalizing a Grayscale Image\n",
    "\n",
    "def normalize_grayscale(image_data):\n",
    "    \n",
    "    a = 0.1\n",
    "    b = 0.9\n",
    "    x_min = 0\n",
    "    x_max = 255\n",
    "    \n",
    "    X = a + ((image_data-x_min)*(b-a))/(x_max-x_min)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Features, labels, and weights\n",
    "\n",
    "features_count = 784\n",
    "labels_count = 10\n",
    "\n",
    "# TODO: Set the features and labels tensors\n",
    "features = tf.placeholder(tf.float32, [None, features_count])\n",
    "labels = tf.placeholder(tf.float32, [None, labels_count])\n",
    "\n",
    "# TODO: Set the weights and biases tensors\n",
    "weights = tf.Variable(tf.truncated_normal((features_count, labels_count)))\n",
    "biases = tf.Variable(tf.zeros(labels_count))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
